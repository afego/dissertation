\chapter{Fundamentação Teórica}\label{cap:fundamentação}

Este capítulo apresenta de forma sucinta os conceitos necessários para compreender as ferramentas utilizadas no capítulo \ref{cap:metodologia}.
Para tal, o capítulo está dividido em duas seções: a primeira aborda conceitos fundamentais de processamento de imagem,
enquanto a segunda seção é dedicada exclusivamente às redes neurais convolucionais e às arquiteturas empregadas.

\section{Técnicas de Processamento de Imagens}

Este trabalho foi desenvolvido em Python, utilizando a biblioteca \textit{Pillow} para manipulação de imagens.
As equações que definem as conversões entre os espaços de cor são apresentadas conforme descrito na documentação do \textit{Pillow} \cite{clark2015pillow}.

\subsection{Espaço de Cores}
Uma imagem colorida é composta por uma ou mais bandas (ou canais).
Usualmente, uma imagem digital colorida é formada por três bandas monocromáticas, sendo que cada banda armazena uma informação distinta da imagem colorida para o pixel p(m,n) naquela banda.

Cada banda, ou canal, de largura MM pixels e altura NN pixels, pode ser concebida como um plano bidimensional, onde um pixel é associado a um par de coordenadas inteiras p(m,n) pertencentes àquela banda.
O valor armazenado na posição (m,n) indica a intensidade da banda naquele ponto da imagem, geralmente codificada em 256 níveis possíveis (valores de 0 a 255) armazenados em um byte.
Nesse contexto, o valor 0 corresponde à intensidade mínima (preto ou ausência de luz) e 255 à intensidade máxima (branco ou máxima luminosidade) \cite{jaelim, shapiro}.

Existem diferentes espaços para representação de cores, sendo o mais comum o RGB-CIE, definido pela \textit{Commission Internationale de l'Éclairage} (CIE) em 1931, ou simplesmente RGB (do inglês \textit{Red, Green and Blue}).
Neste espaço, cada cor é representada em seu componente espectral primário correspondente, de modo que cada banda armazena exclusivamente a intensidade do vermelho, verde ou azul, respectivamente \cite{gonzalez, jaelim}.

Outro espaço importante é o XYZ-CIE 1931 (abreviado como XYZ), projetado para representar todas as cores visíveis, incluindo aquelas que não podem ser representadas no espaço RGB.
As componentes XX e ZZ correspondem a elementos sem correspondência direta na percepção visual humana, não existindo, portanto, como cores perceptíveis na prática.
Já a componente YY representa a luminância, ou seja, a melhor representação monocromática possível da cena originalmente colorida \cite{ohta}.
% -----------------------

\subsection{Conversões entre RGB e YCbCr}
% https://github.com/python-pillow/Pillow/blob/61a35f94cf8a217db3e67d32db943b05e593e781/src/libImaging/ConvertYCbCr.c#L17-L21

Nas conversões entre espaços de cor, não ocorre qualquer alteração prévia nos valores dos pixels no espaço de cor original antes de serem convertidos para o espaço de cor desejado.

As equações \ref{eqn:ycrcb_y}, \ref{eqn:ycrcb_cb} e \ref{eqn:ycrcb_cr} apresentam a transformação do espaço RGB para o espaço YCbCr.
Por sua vez, as equações \ref{eqn:rgb_r}, \ref{eqn:rgb_g} e \ref{eqn:rgb_b} descrevem a conversão inversa, do espaço YCbCr para RGB.

\begin{equation}
    \label{eqn:ycrcb_y}
    Y \xleftarrow{} R \cdot 0,29900 + G \cdot 0,58700 + B \cdot 0,11400
\end{equation}
\begin{equation}
    \label{eqn:ycrcb_cb}
    Cb \xleftarrow{} R \cdot -0,16874 + G \cdot -0,33126 + B \cdot 0,50000 + 128
\end{equation}
\begin{equation}
    \label{eqn:ycrcb_cr}
    Cr \xleftarrow{} R \cdot 0,50000 + G \cdot -0,41869 + B \cdot - 0,08131 + 128
\end{equation}

\begin{equation}
    \label{eqn:rgb_r}
    R \xleftarrow{} Y + (Cr - 128) \cdot 1.40200
\end{equation}
\begin{equation}
    \label{eqn:rgb_g}
    G \xleftarrow{} Y + (Cb - 128) \cdot -0,34414 + (Cr - 128) \cdot -0,71414
\end{equation}
\begin{equation}
    \label{eqn:rgb_b}
    B \xleftarrow{} Y + (Cb - 128) \cdot 1,77200
\end{equation}

A conversão para YCbCr foi realizada com o intuito de utilizar o canal Y para corrigir a intensidade luminosa das imagens, como comentado na seção \ref{subsec:datapreprocessing}.

% -----------------------
\subsection{Transformações de Intensidade no Domínio Espacial}

As transformações de intensidade são funções matemáticas no domínio espacial da imagem, que operam diretamente no valor dos pixels da imagem de entrada \cite{gonzalez}.
Essas funções podem ser representadas pela expressão:
\begin{equation}
    \label{eqn:intensitytransformation}
    g(x,y) = T\left [ f(x,y) \right ]
\end{equation}
onde \textit{f(x,y)} é a imagem de entrada, \textit{g(x,y)} é a imagem de saída e \textit{T} é uma função definida sobre um ponto \textit{(x,y)} ou sua vizinhança.

Uma das transformações de intensidade mais simples é a transformação linear, onde os valores de intensidade de um canal são multiplicados por uma constante, como representado pela equação:
\begin{equation}
    \label{eqn:linearintensitytransformation}
    i^{'} = \alpha \times i
\end{equation}

Através da equação \ref{eqn:linearintensitytransformation}, a intensidade \textit{i} de um pixel é multiplicada pelo fator de escala $\alpha$, resultando na nova intensidade  $i^{'}$.

% -----------------------
\subsection{Filtragem por Convolução}

Também denominados máscaras ou kernels, os filtros consistem em matrizes de dimensão $m \times n$ que são aplicadas sobre uma imagem por meio de operações de multiplicação elemento a elemento entre os valores da máscara e os pixels correspondentes da imagem, seguidas de somas dos resultados. O valor obtido a partir dessa operação corresponde ao pixel processado na imagem resultante.

As dimensões e os valores que compõem essas máscaras variam de acordo com a característica específica que se deseja extrair de cada vizinhança $m \times n$ da imagem, tais como bordas, texturas ou outras propriedades locais \cite{shapiro}.
% ----------------------------------------------------------
\section{Redes Neurais Convolucionais}

As redes neurais convolucionais, ou \acrshort{cnn}s, são uma classe especial de redes neurais artificiais que se especializam em processar dados com uma topologia em matrizes \cite{Goodfellow-et-al-2016}. 

Inspiradas pela organização do córtex visual dos mamíferos \cite{hubel1962}, as \acrshort{cnn}s são projetadas para 
extrair características locais que dependem de uma pequena vizinhança na imagem, onde estas pequenas características podem ser usadas por outras camadas para detectar características de maior ordem e extrair informações sobre a imagem \cite{book:Bishop}. 

\acrshort{cnn}s são usadas principalmente para tarefas que trabalham em imagens, como reconhecimento de objetos, segmentação de imagens e detecção de padrões complexos de dados visuais.

O aprendizado de características locais permite que a \acrshort{cnn} reconheça esses padrões em qualquer outro local da imagem, tornando-a invariante a translação. A utilização de características de uma camada por camadas sucessivas permite que a \acrshort{cnn} aprenda padrões espaciais hierárquicos \cite{book:Chollet}.

% -----------------------
\subsection{Arquitetura de uma Rede Neural Convolucional}

A arquitetura básica de uma \acrshort{cnn} inclui três tipos principais de camadas: convolucional, de pooling e totalmente conectada. 
Uma \acrshort{cnn} típica é formada por pares de camadas de convolução e de pooling empilhadas, que ao final são ligadas a uma camada totalmente conectada.
A Figura \ref{fig:cnn_arch} mostra uma arquitetura genérica de \acrshort{cnn}. 

\begin{figure}[htb]
\centerline{\includegraphics[width=1\linewidth]{images/cnn_placeholder.jpg}}
\caption{Arquitetura de uma \acrshort{cnn} genérica.}
\label{fig:cnn_arch}
\end{figure}

% -----------------------
\subsubsection{Camada Convolucional}

A camada convolucional é o núcleo de uma \acrshort{cnn} e é responsável pela extração de características. 
Para isso pesos na forma de kernels, ou filtros da Análise de Imagem tradicional, 
são aplicados sobre a imagem, onde são escorregados e multiplicados pelos valores dos pixels da imagem de entrada e somados resultando em um pixel para cada kernel, 
usando uma quantidade definida de passos. 

Esse processo gera os mapas de características, onde cada mapa representa a resposta de um filtro específico a uma região da imagem de entrada. 
Essa camada detecta padrões locais como bordas, texturas ou formas, (de acordo com o kernel usado) que podem ser usados para extrair informações de padrões mais complexos conforme a rede se aprofunda.

A operação de convolução S entre um filtro I e uma entrada K pode ser representada como:

\begin{equation}
    \label{eqn:convolution}
    S(x,y) = \sum_{m}\sum_{n}I(x+m,y+n)K(m,n)
\end{equation}
onde S(x,y) é o mapa de características gerado (saida), I(x,y) representa a entrada local (imagem original) e K(m,n) o kernel aplicado.
% -----------------------
\subsubsection{Camada de Pooling}

A camada de pooling altera a saída da camada convolucional com um resumo estatístico de uma vizinhança, melhorando a eficiência computacional da rede pela diminuição da dimensionalidade da saída e ajudando a tornar a saída menos variante a translações da entrada.
Este resumo estatístico pode ser feito com diferentes operações. Por exemplo a arquitetura VGG19 \cite{vgg} utiliza \textit{max pooling}, onde a saída é o maior valor na vizinhança analisada. A operação \textit{average pooling} é utilizada no InceptionV3 \cite{inceptionv3}, onde a média da vizinhança é calculada.


% -----------------------
\subsubsection{Camada Totalmente Conectada}

Estas são geralmente as últimas camadas em uma \acrshort{cnn}, sendo responsáveis pela combinação das características extraídas nas camadas anteriores para realizar a classificação final. 
Nesta camada, todos os neurônios estão conectados a cada unidade da camada anterior, como um perceptron multicamadas comum \cite{MURTAGH1991183}. 
O seu aprendizado é feito por um algoritmo de retro-propagação de erro \cite{Rumelhart1986LearningRB}.
% -----------------------
\subsection{Processo de Treinamento de uma CNN}

O treinamento de uma \acrshort{cnn} é realizado por meio de aprendizado supervisionado, onde o modelo ajusta os pesos e vieses das camadas convolucionais (totalmente conectadas) para minimizar a diferença entre as previsões e os rótulos verdadeiros.

A função de perda, que calcula o erro da rede, é derivada e propagada para ajustar os pesos de forma iterativa. 
Este trabalho utilizou a Entropia Cruzada como função de perda, que é definida por:

% Retirado de Goodfellow
\begin{equation}
    \label{eqn:crossentropy}
    H(P,Q) = -E_{P}[log Q]
\end{equation}
onde H(P,Q) é a entropia cruzada da distribuição Q em relação a distribuição P, sendo a distribuição Q o resultado da \acrshort{cnn} e a distribuição P a distribuição de treinamento \cite{mao2023}. 
$E_{P}[.]$ é o valor esperado em relação a distribuição P.

Para minimizar a função de perda, foi usada a descida de gradiente estocástica (\acrshort{sgd}, do inglês \acrlong{sgd}) \cite{ruder2016}. Enquanto a descida de gradiente padrão calcula o gradiente utilizando todo o conjunto de dados, o \acrshort{sgd} utiliza amostras de dados para o cálculo do gradiente na iteração atual.
Dessa forma, a \acrshort{sgd} altera o valor dos parâmetros da seguinte forma:
%Retirado de Bishop
\begin{equation}
    \label{eqn:sgd}
    \omega^{\tau+1} = \omega{\tau} - \eta\Delta{E}
\end{equation}
onde $\omega$ é o peso sendo usado, $\tau$ é o número da iteração, $\eta$ é a taxa de aprendizado e $\Delta{E}$ é o gradiente do erro.

% -----------------------
\subsection{Aprendizado por Transferência}
% IBM, Wikipedia e UFSC referenciam por 'Aprendizado por transferência', AWS e UFRJ referenciam por 'Transferência por aprendizado'
Uma possibilidade importante nas \acrshort{cnn}s é o uso de redes pré-treinadas e aprendizado por transferência (\textit{transfer learning}), 
onde os pesos dos modelos pré-treinados em grandes conjuntos de dados são aproveitados e ajustados para problemas específicos com menos dados, 
reduzindo o tempo de treinamento e melhorando a precisão em aplicações com conjuntos de dados menores. 
Neste trabalho, o aprendizado por transferência baseado no IMAGENET \cite{deng2009imagenet} foi aplicado em todos os modelos avaliados, alterando sua última camada para uma classificação binária.
% -----------------------
\subsection{Visual Geometry Group (VGG)}
A \acrshort{vgg} é uma família de \acrshort{cnn}s que possuem arquiteturas simples. 
Propostas por \cite{vgg}, as \acrshort{vgg}s são compostas por blocos uniformes empilhados com duas camadas convolucionais e uma camada de \textit{pooling}. 
As camadas de \textit{pooling} têm tamanho $2 \times 2$ reduzindo a dimensionalidade da convolução pela metade. 

Já as camadas convolucionais compartilham do mesmo tamanho de filtro, sendo estes $3 \times 3$. Além disso, cada bloco dobra a quantidade de filtros de suas camadas convolucionais em relação ao bloco anterior. Por exemplo, o primeiro bloco de uma rede \acrshort{vgg} possui 64 filtros $3 \times 3$ em suas camada convolucionais, enquanto o segundo bloco possui 128 filtros $3 \times 3$.

Este trabalho utilizou a \acrshort{vgg}-19, que indica a existência de 19 camadas de aprendizado, contando não só as camadas convolucionais como as camadas completamente conectadas.
% -----------------------
\subsection{Inception}

Diferente do \acrshort{vgg}, a família \textit{Inception}\cite{inception} de \acrshort{cnn}s utiliza o empilhamento de módulos Inception, onde convoluções de diferentes tamanhos são realizadas no mesmo módulo, especificamente convoluções de $1 \times 1$, $3 \times 3$ e $5 \times 5$. Ao final, tais convoluções são concatenadas junto ao \textit{pooling} da própria entrada, gerando a saída do módulo. 

O \textit{InceptionV3}\cite{inceptionv3} fatoriza convoluções maiores em uma sequência de convoluções menores. A convolução $5 \times 5$, por exemplo, é substituída por duas convoluções $3 \times 3$, diminuindo a quantidade de parâmetros mas mantendo a expressividade das características extraídas.

Outra modificação implementada no \textit{InceptionV3} foi a utilização do \textit{label smoothing}, onde a confiança do modelo é reduzida para evitar \textit{overfitting}. Isso é feito alterando a probabilidade esperada de cada classe:

\begin{equation}
    \label{eqn:labelsmoothing}
     q(k) = (1-\epsilon )\delta _{k,j}+\frac{\epsilon }{K}
\end{equation}

Onde \textit{j} é a classe correta, \textit{k} é a classe sendo avaliada, $\delta_{k,j}$ é o \textit{Delta de Dirac}, que se torna 1 quando \textit{k = j} e 0 em qualquer outro caso, $\epsilon$ é uma probabilidade de incerteza do modelo, e \textit{K} é o número de classes. Portanto, o \textit{label smoothing} retira da probabilidade \textit{1} da classe correta uma incerteza $\epsilon$ e a distribui igualmente entre as outras \textit{K} classes do problema.
% -----------------------
\subsection{DenseNet}

A característica de maior importância da \textit{DenseNet}\cite{densenet} é que sua arquitetura é baseada em blocos densos, 
que são compostos por camadas convolucionais onde sua entrada recebe a saída de todas as camadas convolucionais anteriores a ela. 
Os blocos densos são ligados por camadas de transição, compostas por uma camada de convolução $1 \times 1$ e uma camada de \textit{average pooling} $2 \times 2$. 
A arquitetura de uma \textit{DenseNet} simples pode ser vista na Figura \ref{fig:densenet_arch} \cite{densenet}.

\begin{figure}[htb]
\centerline{\includegraphics[width=1\linewidth]{images/densenet.png}}
\caption{Arquitetura de uma \textit{DenseNet} genérica\cite{densenet}.}
\label{fig:densenet_arch}
\end{figure}

% -----------------------
\subsection{MobileNet}
A família \textit{MobileNet} de redes neurais foi desenvolvida para utilização por aplicações móveis ou embarcadas \cite{mobilenet}, 
posuindo arquiteturas mais simples e leves em comparação a outras redes neurais profundas, 
mas continuam a oferecer boa performance. Sua maior distinção de outras \acrshort{cnn}s é a introdução de convoluções separáveis em profundidade, que substituem as convoluções tradicionais.

As convoluções separáveis em profundidades fatorizam uma convolução comum em uma convolução em profundidade e uma convolução 1$\times$1, também chamada de convolução pontual. Isso significa que na convolução em profundidade, em vez de aplicar um filtro que abrange todos os canais da janela a qual está realizando a convolução, será aplicada um filtro por canal, onde a saída de cada canal é combinada pela convolução pontual, reduzindo o custo computacional envolvido na convolução.

Além disso, a arquitetura também possui dois hiperparâmetros globais, um multiplicador de largura $\alpha$ e um multiplicador de resolução $\rho$, que servem para reduzir o custo computacional da rede. O hiperparâmetro $\alpha$ serve para afinar a rede neural em cada camada por um fator $\alpha$, aplicado tanto no número de canais de entrada quanto de canais de saída. Já o hiperparâmetreo $\rho$ é aplicado na imagem de entrada e nas representações internas de cada camada, multiplicando-os por $\rho$.

A arquitetura \textit{MobileNetV2}\cite{mobilenetv2} introduz blocos inversos residuais na arquitetura da rede. Este bloco aumenta a dimensionalidade da entrada através de um conjunto de convoluções 1$\times$1 e aplica uma convolução em profundidade em seguida, para capturar informações em cada canal expandido. Ao final, uma convolução 1$\times$1 é aplicada para reduzir os canal de volta ao tamanho original.

% -----------------------
\subsection{Visual Transformer (ViT)}
A arquitetura \textit{Visual Transformer}\cite{vit} aplica o conceito de \textit{transformers}\cite{transformer2017} 
utilizados em processamento de linguagem natural para tarefas de classificação de imagens, como alternativa às \acrshort{cnn}s. 
A arquitetura em si é relativamente simples, composta um \textit{transformer enconder}, sem a utilização de um \textit{transformer decoder}, 
que fornece sua saída a um perceptron multicamadas simples (\acrshort{mlp}, do inglês \acrlong{mlp}), sendo este o responsável pela classificação. 
Apesar de simples, nos processos necessários para enviar a imagem para o \textit{transformer} é onde a complexidade da arquitetura aparece.

A Figura \ref{fig:vit_arch} \cite{vit}, mostra a arquitetura de um \acrshort{vit}. 
Diferente de uma \acrshort{cnn} comum, a imagem é dividida em blocos de tamanho fixo denominados \textit{patches}. 
Esses \textit{patches} são transformados em vetores e linearizados. 
Para convertê-los ao espaço de características do modelo, os \textit{patches} passam por um \textit{embedding}, através de uma projeção em um espaço de dimensão fixa. 
Antes de enviar a sequência de \textit{embeddings} ao \textit{transformer}, 
é necessário adicionar um \textit{token} de posicionamento para informar à rede sobre a posição de cada \textit{patch} na imagem original. 
Além desses \textit{token}s, um \textit{token} adicional \textit{class} é colocado no início da sequência, que receberá a saída do processamento realizado pelo \textit{transformer}.

\begin{figure}[htb]
\centerline{\includegraphics[width=1\linewidth]{images/visual transformer.png}}
\caption{Arquitetura de um \textit{Visual Transformer}\cite{vit}.}
\label{fig:vit_arch}
\end{figure}

O \textit{Transformer} é uma sequência de pares de camadas alternadas, o tamanho dessa sequência de camadas define a profundidade do modelo. 
O primeiro par de camadas consiste em uma camada de normalização, transformando os valores de todos os elementos da sequência para limites predefinidos, e uma camada de \textit{Multi-Headed Self-Attention}, onde é computada uma soma ponderada de entre cada par de elemento do \textit{patch}.
O segundo par de camadas possui uma camada de normalização e uma camada de \acrshort{mlp}, para processar cada elemento do \textit{patch}.

O processamento final do \textit{transformer} é enviado para um \acrshort{mlp} final denominado \textit{MLP Head}, responsável por receber o \textit{token} \textit{class} que representa as informações da imagem aprendidas no \textit{transformer}, e utilizá-lo para gerar a classificação final da imagem.